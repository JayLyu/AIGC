<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[AIGC]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://aigc.baikong.app/</link><image><url>https://aigc.baikong.app/lib/media/favicon.png</url><title>AIGC</title><link>https://aigc.baikong.app/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 17 Mar 2024 14:52:54 GMT</lastBuildDate><atom:link href="https://aigc.baikong.app/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 17 Mar 2024 14:52:52 GMT</pubDate><copyright><![CDATA[LyuJie]]></copyright><ttl>60</ttl><dc:creator>LyuJie</dc:creator><item><title><![CDATA[前期准备]]></title><description><![CDATA[ 
 <br><br>Step 1. 打开 <a data-tooltip-position="top" aria-label="https://docs.conda.io/en/latest/miniconda.html" rel="noopener" class="external-link" href="https://docs.conda.io/en/latest/miniconda.html" target="_blank">Miniconda</a>，下载并安装对应的 <a data-href="Conda" href="https://aigc.baikong.app/300-aigc/390-glossary/conda.html" class="internal-link" target="_self" rel="noopener">Conda</a> 版本。<br><br>
注意：MacOS(Intel) 不在考虑范围之内
<br>Step 2. 创建一个名为 aigc 、Python 版本为 3.10 的虚拟环境<br>conda create --name aigc python=3.10
Copy<br>Step 3. 激活虚拟环境<br>conda activate comfyui
Copy<br>Step 4. 更新 pip 包管理器<br>pip install --upgrade pip
Copy<br>Step 5. 退出虚拟环境<br>conda deactivate
Copy]]></description><link>https://aigc.baikong.app/300-aigc/310-prepare/前期准备.html</link><guid isPermaLink="false">300-AIGC/310-Prepare/前期准备.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 08:52:21 GMT</pubDate></item><item><title><![CDATA[wd14-tagger]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://github.com/picobyte/stable-diffusion-webui-wd14-tagger" rel="noopener" class="external-link" href="https://github.com/picobyte/stable-diffusion-webui-wd14-tagger" target="_blank">stable-diffusion-webui-wd14-tagger</a>，反推 Tag 插件<br><br>git clone https://github.com/picobyte/stable-diffusion-webui-wd14-tagger.git extensions/tagger
Copy<br><br>如果模型自动下载失败，需要手动下载并存放在指定路径：<br>stable-diffusion-webui/models/interrogators
- model.json
- .locks/
	- models--SmilingWolf--wd-v1-4-moat-tagger-v2/
- models--SmilingWolf--wd-v1-4-moat-tagger-v2/
	- blobs/
	- refs/
		- main
	- snapshots/
		- 8452cddf280b952281b6e102411c50e981cb2908/
			- model.onnx
			- selected_tags.csv
Copy<br>其中 main 文件内容为 8452cddf280b952281b6e102411c50e981cb2908。<br>model.json 内容为：<br>[
  {
    "name": "WD14 ViT v2",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-vit-tagger-v2\\snapshots\\8452cddf280b952281b6e102411c50e981cb2908\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-vit-tagger-v2\\snapshots\\8452cddf280b952281b6e102411c50e981cb2908\\selected_tags.csv"
  },
  {
    "name": "WD14 moat tagger v2",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-moat-tagger-v2\\snapshots\\8452cddf280b952281b6e102411c50e981cb2908\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-moat-tagger-v2\\snapshots\\8452cddf280b952281b6e102411c50e981cb2908\\selected_tags.csv"
  },
  {
    "name": "WD14 ViT v2",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-vit-tagger-v2\\snapshots\\1f3f3e8ae769634e31e1ef696df11ec37493e4f2\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-vit-tagger-v2\\snapshots\\1f3f3e8ae769634e31e1ef696df11ec37493e4f2\\selected_tags.csv"
  },
  {
    "name": "WD14 ConvNeXT v1",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-convnext-tagger\\snapshots\\4036ca51f1c082b0e7c4496890bbf9eadad5764a\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-convnext-tagger\\snapshots\\4036ca51f1c082b0e7c4496890bbf9eadad5764a\\selected_tags.csv"
  },
  {
    "name": "WD14 ConvNeXT v2",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-convnext-tagger-v2\\snapshots\\4b34d1b07bdd8e95494072648960b8a6adcbc0ff\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-convnext-tagger-v2\\snapshots\\4b34d1b07bdd8e95494072648960b8a6adcbc0ff\\selected_tags.csv"
  },
  {
    "name": "WD14 ConvNeXTV2 v1",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-convnextv2-tagger-v2\\snapshots\\dbd4dbe553ee51feb3bc745b614fb762080e3912\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-convnextv2-tagger-v2\\snapshots\\dbd4dbe553ee51feb3bc745b614fb762080e3912\\selected_tags.csv"
  },
  {
    "name": "WD14 SwinV2 v1",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-swinv2-tagger-v2\\snapshots\\e8a736126633b7e60d0ce59930ee8b70642d7560\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-swinv2-tagger-v2\\snapshots\\e8a736126633b7e60d0ce59930ee8b70642d7560\\selected_tags.csv"
  },
  {
    "name": "WD14 ViT v1",
    "model_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-vit-tagger\\snapshots\\213a7bd66d93407911b8217e806a95edc3593eed\\model.onnx",
    "tags_path": "E:\\stable-diffusion-webui\\models\\interrogators\\models--SmilingWolf--wd-v1-4-vit-tagger\\snapshots\\213a7bd66d93407911b8217e806a95edc3593eed\\selected_tags.csv"
  }
]
Copy]]></description><link>https://aigc.baikong.app/300-aigc/320-sdwebui/extensions/wd14-tagger.html</link><guid isPermaLink="false">300-AIGC/320-SDWebUI/Extensions/wd14-tagger.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 09:57:24 GMT</pubDate></item><item><title><![CDATA[如何安装 SD WebUI]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://github.com/AUTOMATIC1111/stable-diffusion-webui" rel="noopener" class="external-link" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank">stable-diffusion-webui</a> 是基于 Gradio 搭建的 stable diffusion 的 GUI 界面。<br><br>
<br>首先，需要根据 <a data-href="前期准备" href="https://aigc.baikong.app/300-aigc/310-prepare/前期准备.html" class="internal-link" target="_self" rel="noopener">前期准备</a> 的内容，在系统中安装虚拟环境
<br>然后，还需要下载并安装 <a data-tooltip-position="top" aria-label="https://git-scm.com/download/win" rel="noopener" class="external-link" href="https://git-scm.com/download/win" target="_blank">git</a>
<br>提前下载好 Stable Diffusion 模型（没有模型会启动失败）
<br><br>Step 1 克隆仓库<br>进入 Documents 文件夹，将 SD WebUI 克隆到当前文件夹<br>cd documents
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
Copy<br>将模型放到 stable diffusion 文件夹内
存放路径：stable-diffusion-webui/models/stable-diffusion
<br>Step 2 设置默认激活虚拟环境<br>修改 stable-diffusion-webui 文件夹内的 webui-user.bat 文件，确保每次启动时自动激活虚拟环境<br>@echo off

conda activate aigc

set PYTHON=
set GIT=
set VENV_DIR=
set COMMANDLINE_ARGS=

call webui.bat
Copy<br>Step 3 启动 WebUI<br>双击 webui-user.bat 启动。启动过程中，会自动安装所需的依赖。<br><br><br>Step 1 安装 Homebrew<br>打开 Terminal 并输入以下命令开始安装 Homebrew<br>/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
Copy<br>安装完成后，需要关闭 Terminal，或重新打开一个 Terminal 继续输入命令。<br>Step 2 安装 Python 3.10<br>brew install cmake protobuf rust python@3.10 git wget
Copy<br>Step 3 克隆仓库<br>进入 Documents 文件夹，将 SD WebUI 克隆到当前文件夹<br>cd documents
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
Copy<br>将模型放到 stable diffusion 文件夹内
存放路径：stable-diffusion-webui/models/stable-diffusion
<br>Step 4. 激活虚拟环境并启动 WebUI<br>使用 Terminal 进入 WebUI 文件夹，激活虚拟环境并启动 WebUI<br>cd stable-diffusion-webui
conda activate aigc
./webui.sh
Copy<br><br>如果电脑中已经安装了 Homebrew，那么可以跳过 Step 1 和 Step 2，直接克隆仓库并启动 WebUI。]]></description><link>https://aigc.baikong.app/300-aigc/320-sdwebui/如何安装-sd-webui.html</link><guid isPermaLink="false">300-AIGC/320-SDWebUI/如何安装 SD WebUI.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 09:56:31 GMT</pubDate></item><item><title><![CDATA[CLIP Text Encode]]></title><description><![CDATA[ 
 <br>CLIP Text Encode 节点的作用是将文本转换为 <a data-href="U-Net" href="https://aigc.baikong.app/300-aigc/390-glossary/u-net.html" class="internal-link" target="_self" rel="noopener">U-Net</a> 可以理解的格式，即文本的数字表示形式，通常也称为<a data-href="embedding" href="https://aigc.baikong.app/300-aigc/390-glossary/embedding.html" class="internal-link" target="_self" rel="noopener">embedding</a>（文本嵌入）。<br>具体实现方式是将 prompt 输入到 CLIP 文本编码器后，编码器会先将 prompt 拆解为多个 token，然后将每个 <a data-href="token" href="https://aigc.baikong.app/300-aigc/390-glossary/token.html" class="internal-link" target="_self" rel="noopener">token</a> 转化为 embedding。这些 embedding 包含了 token 的语义信息和特征。在生成图像的过程中，这些转换后的 embedding 会被传递给 KSampler 节点进行采样，以指导 Stable Diffusion 模型生成与 prompt 匹配的图像。<br>
Embedding 是将 Token 映射到低纬度向量空间的过程。
<br><br><br>实际该 prompt 会被转化为 6 个 token]]></description><link>https://aigc.baikong.app/300-aigc/330-comfyui/nodes/clip-text-encode.html</link><guid isPermaLink="false">300-AIGC/330-ComfyUI/Nodes/CLIP Text Encode.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 14:32:26 GMT</pubDate></item><item><title><![CDATA[ComfyUI-layerdiffusion]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://github.com/huchenlei/ComfyUI-layerdiffusion" rel="noopener" class="external-link" href="https://github.com/huchenlei/ComfyUI-layerdiffusion" target="_blank">ComfyUI-layerdiffusion</a>可以：<br>
<br>生成透明背景的物品
<br>为透明图生成背景
<br>将背景从图片中分离（两张图必须有一致的背景），或将主体分离出来（必须有相同的背景）
<br>将主体与背景合并
<br># node name: Layer Diffusion Apply
# method: Conv Injection
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/layer_xl_transparent_conv.safetensors" to ComfyUI\models\layer_model\layer_xl_transparent_conv.safetensors

# method: Attention Injection
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/layer_xl_transparent_attn.safetensors" to ComfyUI\models\layer_model\layer_xl_transparent_attn.safetensors

# node name: LayeredDiffusionDecode
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/vae_transparent_decoder.safetensors" to ComfyUI\models\layer_model\vae_transparent_decoder.safetensors

# node name: Layer Diffusion Cond Apply
# layer type: foreground
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/layer_xl_fg2ble.safetensors" to ComfyUI\models\layer_model\layer_xl_fg2ble.safetensors

# node name: Layer Diffusion Cond Apply
# layer type: background
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/layer_xl_bg2ble.safetensors" to ComfyUI\models\layer_model\layer_xl_bg2ble.safetensors

# node name: Layer Diffusion Diff Apply
# layer type: foreground
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/layer_xl_fgble2bg.safetensors" to E:\ComfyUI_windows_portable\ComfyUI\models\layer_model\layer_xl_fgble2bg.safetensors

# node name: Layer Diffusion Diff Apply
# layer type: background
Downloading: "https://huggingface.co/LayerDiffusion/layerdiffusion-v1/resolve/main/layer_xl_bgble2fg.safetensors" to ComfyUI\models\layer_model\layer_xl_bgble2fg.safetensors
Copy]]></description><link>https://aigc.baikong.app/300-aigc/330-comfyui/plugins/comfyui-layerdiffusion.html</link><guid isPermaLink="false">300-AIGC/330-ComfyUI/Plugins/ComfyUI-layerdiffusion.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 14:33:48 GMT</pubDate></item><item><title><![CDATA[如何安装 ComfyUI]]></title><description><![CDATA[ 
 <br><br>
<br>首先，需要根据 <a data-href="前期准备" href="https://aigc.baikong.app/300-aigc/310-prepare/前期准备.html" class="internal-link" target="_self" rel="noopener">前期准备</a> 的内容，在系统中安装虚拟环境
<br><br><br>git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI
Copy<br>激活虚拟环境<br>conda activate aigc
Copy<br>安装 torch<br>pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 xformers
Copy<br>如果出现“Torch not compiled with CUDA enabled”错误，需要先卸载 torch 后再次安装：<br>pip uninstall torch
Copy<br>安装所需依赖：<br>pip install -r requirements.txt
Copy<br><br><br><br><br><br>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
sh Miniconda3-latest-MacOSX-arm64.sh
Copy<br><br><br>conda install pytorch torchvision torchaudio -c pytorch-nightly
Copy<br><br>pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
Copy<br><br>import torch
if torch.backends.mps.is_available():
    mps_device = torch.device("mps")
    x = torch.ones(1, device=mps_device)
    print (x)
else:
    print ("MPS device not found.")
Copy<br>打印的内容为：<br>tensor([1.], device='mps:0')
Copy<br><br>与 Windows 的安装教程一致<br><br>使用 python main.py --force-fp16 命令启动。]]></description><link>https://aigc.baikong.app/300-aigc/330-comfyui/如何安装-comfyui.html</link><guid isPermaLink="false">300-AIGC/330-ComfyUI/如何安装 ComfyUI.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 09:42:34 GMT</pubDate></item><item><title><![CDATA[SD 模型名与文件名对照表]]></title><description><![CDATA[ 
 <br>模型下载：<a data-tooltip-position="top" aria-label="https://huggingface.co/stabilityai" rel="noopener" class="external-link" href="https://huggingface.co/stabilityai" target="_blank">stabilityai</a><br>]]></description><link>https://aigc.baikong.app/300-aigc/340-other/sd-模型名与文件名对照表.html</link><guid isPermaLink="false">300-AIGC/340-Other/SD 模型名与文件名对照表.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 14:34:57 GMT</pubDate></item><item><title><![CDATA[CLIP]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://github.com/openai/CLIP" rel="noopener" class="external-link" href="https://github.com/openai/CLIP" target="_blank">openai/CLIP</a> 是一种 <a data-href="Text-to-image Model" href="https://aigc.baikong.app/300-development/deep-learning/text-to-image-model.html" class="internal-link" target="_self" rel="noopener">Text-to-image Model</a>，可以根据文本描述检索或生成图像。CLIP 使用类似 ViT 的视觉变换器来获取视觉特征，并使用因果语言模型来获取文本特征。然后将文本和视觉特征投影到具有相同维度的潜在空间。<br>CLIP 由 OpenAI 开发。<br>CLIP 使用了大量互联网数据来训练一个视觉变换器（ViT）和一个文本编码器。
CLIP 使用类似 ViT<a href="https://aigc.baikong.app/#fn-1-8121bf1a974b82f7" class="footnote-link" target="_self" rel="noopener">[1]</a> 的视觉变换器来获取视觉特征，并使用因果语言模型来获取文本特征。然后将文本和视觉特征投影到具有相同维度的潜在空间。
<br>CLIP 可以和其他图像生成模型结合，比如 <a data-href="DALL-E" href="https://aigc.baikong.app/300-development/deep-learning/dall-e.html" class="internal-link" target="_self" rel="noopener">DALL-E</a>、<a data-href="ControlNet" href="https://aigc.baikong.app/300-development/stable-diffusion/controlnet.html" class="internal-link" target="_self" rel="noopener">ControlNet</a>，来实现更高质量的图像生成。<br><img alt="CLIP_Contrastive_pre_training.svg" src="https://aigc.baikong.app/lib/media/clip_contrastive_pre_training.svg"><br>
CLIP 预训练一个图像编码器和一个文本编码器来预测哪些图像与我们数据集中的哪些文本配对。然后我们使用这种行为将 CLIP 变成零样本分类器。我们将数据集的所有类别转换为标题，例如“狗的照片”，并预测标题的类别 CLIP 估计与给定图像的最佳配对。
<br><img alt="CLIP_Create_dataset_classifier_from_label_text.svg" src="https://aigc.baikong.app/lib/media/clip_create_dataset_classifier_from_label_text.svg"><br>Reference

<br><a data-tooltip-position="top" aria-label="https://openai.com/research/clip" rel="noopener" class="external-link" href="https://openai.com/research/clip" target="_blank">Openai - CLIP</a>
<br><a data-tooltip-position="top" aria-label="https://morris-lee.medium.com/computer-vision-transformer-models-clip-vit-deit-released-by-hugging-face-31f475a440ac" rel="noopener" class="external-link" href="https://morris-lee.medium.com/computer-vision-transformer-models-clip-vit-deit-released-by-hugging-face-31f475a440ac" target="_blank">Hugging Face 发布的计算机视觉转换器模型（CLIP、ViT、DeiT）</a>
<br><a data-tooltip-position="top" aria-label="https://huggingface.co/docs/transformers/model_doc/clip" rel="noopener" class="external-link" href="https://huggingface.co/docs/transformers/model_doc/clip" target="_blank">huggingface - clip</a>

<br>
<br>
<br>
Vision Transformer, by Google AI
<a href="https://aigc.baikong.app/#fnref-1-8121bf1a974b82f7" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>

]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/clip.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/CLIP.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 13:59:09 GMT</pubDate><enclosure url="https://aigc.baikong.app/lib/media/clip_contrastive_pre_training.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://aigc.baikong.app/lib/media/clip_contrastive_pre_training.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Conda]]></title><description><![CDATA[ 
 <br><br>由 <a data-tooltip-position="top" aria-label="https://conda.io" rel="noopener" class="external-link" href="https://conda.io" target="_blank">Anaconda</a> 发布的开源虚拟环境和包管理工具，可以实现跨平台的包管理和环境管理，解决依赖关系问题。]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/conda.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/Conda.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 08:42:07 GMT</pubDate></item><item><title><![CDATA[embedding]]></title><description><![CDATA[ 
 <br>embedding 是文本的数字化表示，用于帮助模型理解用户的提示并指导 Stable Diffusion 模型生成与提示匹配的图像。<br>embedding 的原理是将每个数据点映射到一个低维度的 embedding 向量中，使得相似的数据点在嵌入空间中距离接近。具体来说，嵌入向量是一个实数向量，通常包含多个元素，每个元素代表一个特征或语义信息。]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/embedding.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/embedding.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 10:06:27 GMT</pubDate></item><item><title><![CDATA[IPAdapter]]></title><description><![CDATA[ 
 <br><br>标黄的两个模型使用了 LAION 的视觉编码器]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/ipadapter.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/IPAdapter.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 21 Jan 2024 10:41:45 GMT</pubDate></item><item><title><![CDATA[Pytorch]]></title><description><![CDATA[ 
 <br>查看 pytorch 版本：<br># 进入 Python 环境
python
Copy<br>import torch
torch.__version__
Copy]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/pytorch.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/Pytorch.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 10:02:40 GMT</pubDate></item><item><title><![CDATA[Tensor]]></title><description><![CDATA[ 
 <br>
张量（英语：Tensor）是一个可用来表示在一些向量、标量和其他张量之间的线性关系的多线性函数，这些线性关系的基本例子有内积、外积、线性映射以及笛卡儿积。其坐标在  维空间内，有   个分量的一种量，其中每个分量都是坐标的函数，而在坐标变换时，这些分量也依照某些规则作线性变换。 称为该张量的秩或阶（与矩阵的秩和阶均无关系）。
<br>Tensor 实际上就是一个多维数组<a href="https://aigc.baikong.app/#fn-1-8121bf1a974b82f7" class="footnote-link" target="_self" rel="noopener">[1]</a>，Tensor 的目的是能够创造更高维度的矩阵、向量。<br>在同构的意义下：<br>
<br>第0阶张量（）为<a data-href="标量" href="https://aigc.baikong.app/标量" class="internal-link" target="_self" rel="noopener">标量</a>
<br>第1阶张量（）为<a data-href="Vector" href="https://aigc.baikong.app/300-development/deep-learning/mathematics/vector.html" class="internal-link" target="_self" rel="noopener">Vector</a><a href="https://aigc.baikong.app/#fn-2-8121bf1a974b82f7" class="footnote-link" target="_self" rel="noopener">[2]</a>
<br>第2阶张量（）为矩阵
<br>例如，对于三维空间， 时的张量为此向量 。<br>例如，任意一张彩色图片用三阶张量来表示，三个维度分别是 。如果用张量表示出来，如下图：<br><img alt="Color_picture_tensor_diagram.jpeg" src="https://aigc.baikong.app/lib/media/color_picture_tensor_diagram.jpeg"><br>其中表格横轴表示图片宽度值，纵轴表示图片高度值，每个方格表示一个像素点。例如，第一行第一列的数据为 ，代表 RGB 三原色在图片的这个位置取值为 。<br>Reference

<br><a data-tooltip-position="top" aria-label="https://zh.wikipedia.org/wiki/%E5%BC%B5%E9%87%8F" rel="noopener" class="external-link" href="https://zh.wikipedia.org/wiki/%E5%BC%B5%E9%87%8F" target="_blank">张量</a>
<br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/20695804" rel="noopener" class="external-link" href="https://www.zhihu.com/question/20695804" target="_blank">什么是张量 (tensor)？</a>
<br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/48982978" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/48982978" target="_blank">笔记 | 什么是张量（tensor）&amp; 深度学习</a>

<br>
<br>
<br>
Multidimensional array
<a href="https://aigc.baikong.app/#fnref-1-8121bf1a974b82f7" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>

<br>
矢量
<a href="https://aigc.baikong.app/#fnref-2-8121bf1a974b82f7" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>

]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/tensor.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/Tensor.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 21 Jan 2024 09:30:01 GMT</pubDate><enclosure url="https://aigc.baikong.app/lib/media/color_picture_tensor_diagram.jpeg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://aigc.baikong.app/lib/media/color_picture_tensor_diagram.jpeg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[token]]></title><description><![CDATA[ 
 <br>在自然语言处理中，文本会被分解成不同的 token，每个 token 代表文本中的一个基本语义单位。<br>在 <a data-href="CLIP" href="https://aigc.baikong.app/300-aigc/390-glossary/clip.html" class="internal-link" target="_self" rel="noopener">CLIP</a> 和其他文本处理任务中，token 是指输入文本被分解成的基本单位， 可以是单词、子词或字符等级，用于构建文本的表示形式。通过对这些 token 进行编码和嵌入，可以将文本转换为数值表示形式，以便模型能够理解和处理文本信息。]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/token.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/token.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 14:10:18 GMT</pubDate></item><item><title><![CDATA[Torchvision]]></title><description><![CDATA[ 
 <br>查看 Torchvision 版本：<br># 进入 Python 环境
python
Copy<br>import torchvision
torchvision.__version__
Copy]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/torchvision.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/Torchvision.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 10:02:29 GMT</pubDate></item><item><title><![CDATA[Transformer]]></title><description><![CDATA[ 
 <br><br>一种 Deep Learning 模型，它采用了自我关注机制，可以将输入的文本提示转换为 token 嵌入向量。]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/transformer.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/Transformer.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sat, 16 Mar 2024 08:42:03 GMT</pubDate></item><item><title><![CDATA[U-Net]]></title><description><![CDATA[ 
 <br>U-Net 基于<a data-href="Convolutional Neural Networks" href="https://aigc.baikong.app/300-development/deep-learning/convolutional-neural-networks.html" class="internal-link" target="_self" rel="noopener">Convolutional Neural Networks</a>，是一种分割模型，可以实现对图像进行语义分割，例如自动驾驶、自动抠图等。<br><img alt="unet-architecture.png" src="https://aigc.baikong.app/lib/media/unet-architecture.png"><br>U-Net 前半部分就是分割图像、特征提取，后半部分是上采样，基本上相当于是 Encoder - Decoder 的结构，由于形状类似 U 形，所以叫 U-Net。<br>U-Net 属于老模型，在 2015 年提出，初衷是解决医学图像分割问题。<br>Reference

<br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/313283141" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/313283141" target="_blank">图像分割必备知识点 | Unet详解 理论+ 代码</a>
<br><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/U-Net" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/U-Net" target="_blank">U-Net - wiki</a>
<br><a data-tooltip-position="top" aria-label="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" rel="noopener" class="external-link" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank">U-Net - Uni Freiburg</a>

]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/u-net.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/U-Net.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 14:03:11 GMT</pubDate><enclosure url="https://aigc.baikong.app/lib/media/unet-architecture.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://aigc.baikong.app/lib/media/unet-architecture.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[虚拟环境]]></title><description><![CDATA[ 
 <br><br>conda&nbsp;是由 <a data-tooltip-position="top" aria-label="https://conda.io" rel="noopener" class="external-link" href="https://conda.io" target="_blank">Anaconda</a> 发布的开源虚拟环境和包管理工具，可以实现跨平台的包管理和环境管理，解决依赖关系问题。<br>conda 的依赖一般安装在 conda 软件目录下。<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.conda.io/en/latest/miniconda.html" rel="noopener" class="external-link" href="https://docs.conda.io/en/latest/miniconda.html" target="_blank">Miniconda</a>：轻量级版本，只包含 conda 工具和一些基本包
<br><a data-tooltip-position="top" aria-label="https://docs.anaconda.com/anaconda/install/" rel="noopener" class="external-link" href="https://docs.anaconda.com/anaconda/install/" target="_blank">Anaconda</a>：大型科学计算发行版，包含了许多科学计算和数据分析相关的包
<br><br>查看已有 conda 环境：<br>conda env
Copy<br>创建 conda 环境：<br>conda create --name &lt;环境名称&gt; &lt;要安装的包&gt;
# Example
conda create --name myenv python=3.8 numpy pandas
Copy<br>--name可以简写为 -n<br>激活虚拟环境：<br>conda activate &lt;环境名称&gt;
# Example
conda activate myenv
Copy<br>安装包：<br>conda install &lt;包名&gt;
# Example
conda install requests
Copy<br>查看已安装的包：<br>conda list
Copy<br>退出虚拟环境：<br>conda deactivate
Copy<br>环境配置文件：<br># 创建环境配置文件
conda env export &gt; environment.yml
# 根据配置文件创建环境
conda env create -f environment.yml
Copy<br>卸载环境：<br>conda env remove --name &lt;环境名称&gt;
# Example
conda env remove --name myenv
Copy]]></description><link>https://aigc.baikong.app/300-aigc/390-glossary/venv.html</link><guid isPermaLink="false">300-AIGC/390-Glossary/venv.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Fri, 15 Mar 2024 14:27:05 GMT</pubDate></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br>目录结构：<br>
<br>10_Prepare：前期准备
<br>20_SD_WebUI：Stable Diffusion WebUI 相关
<br>30_ComfyUI：ComfyUI 相关
<br>40_Other：暂时无法归类的内容
<br>90_Glossary：词汇表
<br><br><br><br><br><br><br><br>]]></description><link>https://aigc.baikong.app/300-aigc/index.html</link><guid isPermaLink="false">300-AIGC/index.md</guid><dc:creator><![CDATA[LyuJie]]></dc:creator><pubDate>Sun, 17 Mar 2024 14:36:46 GMT</pubDate></item></channel></rss>